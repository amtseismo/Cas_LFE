#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Created on Fri Nov 20 15:55:16 2020

@author: timlin

6/1 2022: merge h5 files and create .csv file

"""

import h5py
import matplotlib.pyplot as plt
import numpy as np
import glob
import pandas as pd
from obspy import UTCDateTime

# Data path
data_path_P = "/projects/amt/jiunting/Cascadia_LFE/Data_QC_rmean_norm/ID_*_*.*.*_P.h5"
data_path_S = "/projects/amt/jiunting/Cascadia_LFE/Data_QC_rmean_norm/ID_*_*.*.*_S.h5"

files_P = glob.glob(data_path_P)
files_S = glob.glob(data_path_S)
files_P.sort()
files_S.sort()

def get_catalog(catalog_file: str) -> pd.DataFrame:
    head = ['famID','mag','catOT']
    sav_all = []
    with open(detcFile,'r') as IN1:
        for line in IN1.readlines():
            line = line.strip()
            ID = line.split()[0] #family ID
            OT = UTCDateTime('20'+line.split()[1]) #YYMMDD
            HH = (int(line.split()[2])-1)*3600  #HR from 1-24
            SS = float(line.split()[3])
            OT = OT + HH + SS  #detected origin time. always remember!!! this is not the real OT. The shifted time in the sav_family_phases.npy have been corrected accordingly.
            Mag = float(line.split()[4])
            sav_all.append([ID,Mag,UTCDateTime(OT).strftime('%Y-%m-%dT%H:%M:%S.%f')[:-2]])
    df = pd.DataFrame(sav_all, columns = head)
    return df


def h5py_merge(h5py_file: str, catalog: pd.DataFrame, out_csv: str, out_h5: str, overwrite: bool=False) -> pd.DataFrame:
    """
    Read individual h5py file and write a big merged h5py and csv file
    
    Inputs
    ======
    h5py_file: str
        File name for individual h5py file (generated from get_data.py)
    catalog: pd.DataFrame
        Catalog generated by get_catalog. To be used to search the idxCatalog
    out_csv: str
        Output name for merged csv file. The file will be saved in the same direcory of individual h5py file.
        If file exist, new data will be appended after the existing file unless specify overwrite=True.
    out_h5: str
        Output name for the big merged h5py file. The file will be saved in the same direcory of individual h5py file
    
    Outputs
    =======
    None
    
    """
    
    l = len(h5py_file.split('/'))
    merged_h5 = "/".join(h5py_file.split('/')[:-1]) if l>1 else "." # prepanding path
    merged_h5 = merged_h5 + "/" + "%s.h5"%(out_csv)
    merged_csv = merged_h5.replace("%s.h5"%(out_csv),"%s.csv"%(out_csv))
    
    if overwrite:
        mode='w'
    else:
        mode='a'
    
    f5out = h5py.File(merged_h5,mode)
    famID = h5py_file.split('/')[-1].split('_')[1]
    net = h5py_file.split('/')[-1].split('_')[2].split('.')[0]
    sta = h5py_file.split('/')[-1].split('_')[2].split('.')[1]
    comp = h5py_file.split('/')[-1].split('_')[2].split('.')[2]
    PS = h5py_file.split('/')[-1].split('_')[3].split('.')[0]
    comb = '.'.join([famID,net,sta,comp,PS]) #to be combined with OT as an unique key
    prepand = [famID, net, sta, comp, PS ]
    all_data = []
    # loop through the h5py file
    fin = h5py.File(h5py_file,'r')
    print(' Number of traces to be written:%d'%(len(fin['waves'])))
    for i,k in enumerate(fin['waves'].keys()):
        if i % (len(fin['waves'])//5) ==0:
            print('  (%f/100 completed)'%( 100*i/(len(fin['waves'])) ))
        kk = '_'.join([k,comb]) #e.g. 2013-10-10T07:19:27.2000 _ 258.PO.SSIB.HH.S
        # write to merged h5 file
        f5out.create_dataset('waves/'+kk,data=fin['waves'][k])
        #find the idx in the catalog
        tmp = catalog[(catalog['famID']==famID) & (catalog['catOT']==k)]
        assert len(tmp)==1, "found more than 1 or not found! %s, %s"%(famID,k)
        idx = tmp.index[0]
        data = prepand + [kk, idx]
        all_data.append(data)
    df = pd.DataFrame(all_data, columns = ['famID', 'net', 'sta', 'comp', 'PS', 'evID' ,'idxCatalog'])
    # write csv file
    if overwrite:
        df.to_csv(merged_csv, header=True, index=False, mode=mode)
    else:
        df.to_csv(merged_csv, header=False, index=False, mode=mode)
    
    fin.close()
    f5out.close()
    return

#LFE detection file
catalog_file = 'total_mag_detect_0000_cull_NEW.txt'
catalog = get_catalog(catalog_file)


# looping P files
for i, fileP in enumerate(files_P):
    print('Now in',i)
    if i==0:
        h5py_merge(fileP, catalog, "merged20220602_P", "merged20220602_P", overwrite=True)
    else:
        h5py_merge(fileP, catalog, "merged20220602_P", "merged20220602_P", overwrite=False)
    if i==2:
        break
    
# looping S files
for i, fileS in enumerate(files_S):
    print('Now in',i)
    if i==0:
        h5py_merge(fileS, catalog, "merged20220602_S", "merged20220602_S", overwrite=True)
    else:
        h5py_merge(fileS, catalog, "merged20220602_S", "merged20220602_S", overwrite=False)
    if i==2:
        break


import sys
sys.exit()


#get all data/noise name
#dataName = glob.glob('/projects/amt/jiunting/Cascadia_LFE/Data/ID_*_*.*.*_S.h5')
dataName = glob.glob('/projects/amt/jiunting/Cascadia_LFE/Data/ID_*_*.*.*_P.h5') 
#noiseName = glob.glob('/projects/amt/jiunting/Cascadia_LFE/Data_noise/*_noise.h5')

#dataName = glob.glob('/projects/amt/jiunting/Cascadia_LFE/Data_QC_rmean/ID_*_*.*.*_S.h5')
#dataName = glob.glob('/projects/amt/jiunting/Cascadia_LFE/Data_QC_rmean_CC_0.1/ID_*_*.*.*_S.h5')
#noiseName = glob.glob('/projects/amt/jiunting/Cascadia_LFE/Data_noise/*_noise.h5')

#small scale test
#dataName = glob.glob('/projects/amt/jiunting/Cascadia_LFE/Data/ID_026_PO.SILB.HH_S.h5')
#noiseName = glob.glob('/projects/amt/jiunting/Cascadia_LFE/Data_noise/PO.T*.h5')

#dataName = glob.glob('/projects/amt/jiunting/Cascadia_LFE/Data_QC_rmean_NotCC_0.2/ID_*_*.*.*_S.h5')

# plots?
plots=0

# Need to merge .h5 files
allData=np.empty((0,9003))
allNoise=np.empty((0,9003))

#deal with data
NData = 0
NDataDrop = 0
NFile = 0
for i_file,lfeFile in enumerate(dataName):
    #allData=np.empty((0,9003)) #instead of stacking new on the whole, save every chunks and stack later
    OUT1 = open('lfe_data.log','a')
    OUT1.write('Now in %d out of %d\n'%(i_file,len(dataName)))
    OUT1.close()
    tmp = h5py.File(lfeFile, 'r')
    data = tmp['waves']
    tmp_allData=np.empty((0,9003)) #data in each .h5 file after QC
    for i_d in data:
        c,tmp_data = QC(i_d,Type='data',return_norm=False) # 2021/9/2 just save the data to Data_QC_rmean(without normalized)
        if c:
            tmp_allData = np.vstack([tmp_allData,tmp_data])
    #====save the data from /projects/amt/jiunting/Cascadia_LFE/Data to /projects/amt/jiunting/Cascadia_LFE/Data_QC_rmean
    #=== batch processing(data have been QC) ===   
    #data = np.array(data)
    #data = data/np.max(np.abs(data),axis=1).reshape(-1,1) #normalize
    #===========================================
    #allData = np.vstack([allData,tmp_allData]) comment this on 2021/9/2
    #allData = np.vstack([allData,data])
    tmp.close()
    h5f = h5py.File('/projects/amt/jiunting/Cascadia_LFE/Data_QC_rmean/'+lfeFile.split('/')[-1], 'w') # cp data from Data to Data_QC_rmean
    h5f.create_dataset('waves', data=tmp_allData)
    h5f.close()
    continue #2021/9/2
    #save every 100 files as a chunk
    if i_file%100==0 and i_file!=0:
        np.save('tmp_%03d.npy'%(i_file),allData)
        #np.save('tmp_%03d.npy'%(NFile),allData)
        allData=np.empty((0,9003)) #reset instead of stacking new on the whole, save every chunks and stack later
    NFile += 1
    '''
    for i_d,d in enumerate(data):
        if QC(d,Type='data'):
            #allData = np.concatenate((allData,d))
            allData = np.vstack([allData,d])
            NData += 1
        else:
            NDataDrop += 1
        #print log file
        if i_d%500==0:
            OUT1 = open('lfe_data.log','a')
            OUT1.write('-- %d out of %d traces\n'%(i_d,len(data)))
            OUT1.close()
    np.save('./tmp_LFE/file_%03d.npy'%(NFile),allData)
    NFile += 1
    '''

#stop here, 2021/9/2
import sys
sys.exit()

if len(allData)!=0:
    np.save('tmp_%03d.npy'%(i_file),allData)

tmp_all = glob.glob('tmp_*.npy')
tmp_all.sort()
allData=np.empty((0,9003))
for i_tmp in tmp_all:
    D = np.load(i_tmp)
    allData = np.vstack([allData,D])


print(allData.shape)
#print('Number of data:',NData)
print('Number of data dropped:',NDataDrop)
#save LFE data
#h5f = h5py.File("Cascadia_lfe_QC_rmean_NotCC0.2.h5", 'w')  
h5f = h5py.File("Cascadia_lfe_QC_rmean_norm_P.h5", 'w')
#h5f = h5py.File("Cascadia_lfe_QC_rmean_norm_check.h5", 'w')  
h5f.create_dataset('waves', data=allData)
h5f.close()

import sys
sys.exit()



#deal with noise
NNoise = 0
NNoiseDrop = 0
NFile = 0
n_traces = 0
allNoise=np.empty((0,9003))
for i_file,noiseFile in enumerate(noiseName):
    #allNoise=np.empty((0,9003))
    OUT1 = open('noise_data.log','a')
    OUT1.write('Now in %d out of %d\n'%(i_file,len(noiseName)))
    OUT1.close()
    tmp = h5py.File(noiseFile, 'r')
    noise = tmp['waves']
    noise = np.array(noise)
    for i_n,n in enumerate(noise):
        if QC(n,Type='noise'):
            #allNoise = np.concatenate((allNoise,n))   
            allNoise = np.vstack([allNoise,n])
            NNoise += 1
            n_traces += 1
        else:
            NNoiseDrop += 1 
        if i_n%500==0:
            OUT1 = open('noise_data.log','a')
            OUT1.write('-- %d out of %d traces\n'%(i_n,len(noise)))
            OUT1.close()
        if n_traces==2500:
            np.save('./tmp_LFE/noise_%03d.npy'%(NFile),allNoise)
            NFile += 1
            n_traces = 0 
            allNoise = np.empty((0,9003))

if allNoise.shape[0]!=0:
    np.save('./tmp_LFE/noise_%03d.npy'%(NFile),allNoise)



#print(allNoise.shape)
print('Number of data:',NNoise)
print('Number of noise dropped:',NNoiseDrop)
# save noise files
#h5f = h5py.File("Cascadia_noise_data.h5", 'w')  
#h5f.create_dataset('waves', data=allNoise)
#h5f.close()  
